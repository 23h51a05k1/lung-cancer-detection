"""
train.py
--------
Training pipeline for the LungCancerCNN model.

Steps:
  1. Load images from dataset/ (generated by generate_dataset.py)
  2. Preprocess each image (resize, normalize, Gaussian blur, CLAHE)
  3. Split into train / validation sets (80 / 20)
  4. Apply data augmentation on the training set
  5. Compile with Adam + categorical cross-entropy
  6. Train with EarlyStopping + ModelCheckpoint
  7. Save trained model to lung_cancer_model.h5
  8. Print final accuracy & loss summary
"""

import os
import sys
import numpy as np
import cv2
from pathlib import Path
from sklearn.model_selection import train_test_split

import tensorflow as tf
from tensorflow.keras import callbacks, optimizers

from model import build_model, CLASS_NAMES, IMG_SIZE

# ── Reproducibility ────────────────────────────────────────────────────────────
SEED = 42
np.random.seed(SEED)
tf.random.set_seed(SEED)

# ── Hyper-parameters ───────────────────────────────────────────────────────────
DATA_DIR    = "dataset"
MODEL_PATH  = "lung_cancer_model.h5"
EPOCHS      = 20
BATCH_SIZE  = 16
VALIDATION_SPLIT = 0.20
LEARNING_RATE    = 1e-3


# ── Preprocessing ──────────────────────────────────────────────────────────────
def preprocess_image(img_bgr):
    """
    Full preprocessing pipeline applied to every image:
      1. Resize to IMG_SIZE × IMG_SIZE
      2. Gaussian blur (noise reduction)
      3. CLAHE contrast enhancement on luminance channel
      4. Normalize to [0, 1]
    Returns a float32 NumPy array of shape (IMG_SIZE, IMG_SIZE, 3).
    """
    # 1. Resize
    img = cv2.resize(img_bgr, (IMG_SIZE, IMG_SIZE))

    # 2. Gaussian blur
    img = cv2.GaussianBlur(img, (3, 3), 0)

    # 3. CLAHE on LAB luminance channel
    lab   = cv2.cvtColor(img, cv2.COLOR_BGR2LAB)
    l, a, b = cv2.split(lab)
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
    l     = clahe.apply(l)
    lab   = cv2.merge([l, a, b])
    img   = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)

    # 4. Normalize
    img = img.astype(np.float32) / 255.0
    return img


# ── Dataset loading ────────────────────────────────────────────────────────────
def load_dataset(data_dir):
    """Walk class sub-directories and return (images, labels) arrays."""
    images, labels = [], []
    missing = []

    for cls_idx, cls_name in enumerate(CLASS_NAMES):
        cls_path = Path(data_dir) / cls_name
        if not cls_path.exists():
            missing.append(cls_name)
            continue

        img_paths = sorted(cls_path.glob("*.png")) + sorted(cls_path.glob("*.jpg"))
        if not img_paths:
            missing.append(cls_name)
            continue

        for p in img_paths:
            bgr = cv2.imread(str(p))
            if bgr is None:
                continue
            images.append(preprocess_image(bgr))
            labels.append(cls_idx)

        print(f"  Loaded {len(img_paths):4d} images  ←  {cls_name}")

    if missing:
        print(f"\n[ERROR] Missing class directories: {missing}")
        print("  Run `python generate_dataset.py` first.")
        sys.exit(1)

    return np.array(images, dtype=np.float32), np.array(labels, dtype=np.int32)


# ── Augmentation ───────────────────────────────────────────────────────────────
def get_augmentation_layer():
    """Keras Sequential augmentation for training images."""
    return tf.keras.Sequential([
        tf.keras.layers.RandomFlip("horizontal_and_vertical"),
        tf.keras.layers.RandomRotation(0.15),
        tf.keras.layers.RandomZoom(0.15),
        tf.keras.layers.RandomContrast(0.10),
    ], name="augmentation")


# ── Main training ──────────────────────────────────────────────────────────────
def train():
    print("\n=== Lung Cancer CNN – Training Pipeline ===\n")

    # 1. Load data
    print("Loading dataset …")
    X, y = load_dataset(DATA_DIR)
    print(f"\nTotal images: {len(X)}  |  Classes: {len(CLASS_NAMES)}\n")

    # 2. One-hot encode
    y_onehot = tf.keras.utils.to_categorical(y, num_classes=len(CLASS_NAMES))

    # 3. Train / validation split
    X_train, X_val, y_train, y_val = train_test_split(
        X, y_onehot, test_size=VALIDATION_SPLIT, random_state=SEED, stratify=y
    )
    print(f"Train: {len(X_train)}  |  Val: {len(X_val)}\n")

    # 4. Build model
    model = build_model()
    model.summary()

    # 5. Compile
    model.compile(
        optimizer=optimizers.Adam(learning_rate=LEARNING_RATE),
        loss="categorical_crossentropy",
        metrics=["accuracy"],
    )

    # 6. Callbacks
    cb_list = [
        callbacks.EarlyStopping(
            monitor="val_accuracy",
            patience=5,
            restore_best_weights=True,
            verbose=1,
        ),
        callbacks.ModelCheckpoint(
            filepath=MODEL_PATH,
            monitor="val_accuracy",
            save_best_only=True,
            verbose=1,
        ),
        callbacks.ReduceLROnPlateau(
            monitor="val_loss",
            factor=0.5,
            patience=3,
            min_lr=1e-6,
            verbose=1,
        ),
    ]

    # 7. Build augmentation layer & tf.data pipelines
    augment = get_augmentation_layer()

    train_ds = (
        tf.data.Dataset.from_tensor_slices((X_train, y_train))
        .shuffle(len(X_train), seed=SEED)
        .batch(BATCH_SIZE)
        .map(lambda x, y: (augment(x, training=True), y),
             num_parallel_calls=tf.data.AUTOTUNE)
        .prefetch(tf.data.AUTOTUNE)
    )
    val_ds = (
        tf.data.Dataset.from_tensor_slices((X_val, y_val))
        .batch(BATCH_SIZE)
        .prefetch(tf.data.AUTOTUNE)
    )

    # 8. Train
    print("\nStarting training …\n")
    history = model.fit(
        train_ds,
        validation_data=val_ds,
        epochs=EPOCHS,
        callbacks=cb_list,
    )

    # 9. Summary
    best_val_acc = max(history.history["val_accuracy"])
    print(f"\n{'='*45}")
    print(f"  Training complete!")
    print(f"  Best validation accuracy : {best_val_acc * 100:.2f}%")
    print(f"  Model saved to           : {MODEL_PATH}")
    print(f"{'='*45}\n")


if __name__ == "__main__":
    train()
